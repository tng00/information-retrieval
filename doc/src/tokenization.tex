\CWHeader{Лабораторная работа \textnumero 3 \enquote{Токенизация}}

\section*{Описание}
\subsection*{Правила токенизации}
Процесс разбиения текста на токены реализован на C++ в виде утилиты командной строки, работающей как фильтр. Были выработаны следующие правила:
\begin{itemize}
    \item \textbf{Приведение к нижнему регистру:} Все символы кириллицы и латиницы переводятся в нижний регистр.
    \item \textbf{Определение границ токена:} Разделителями считаются пробелы, знаки препинания и символы перевода строки. Они не включаются в состав токена.
    \item \textbf{Состав токена:} Токеном считается непрерывная последовательность букв, цифр и дефиса (если он не в начале или конце слова).
    \item \textbf{Фильтрация:} Отбрасываются токены, состоящие только из цифр или дефисов, а также токены длиной менее двух символов после всех преобразований.
\end{itemize}

\textbf{Достоинства:} Высокая скорость обработки за счет реализации на C++ и использования конечного автомата, простота правил.
\textbf{Недостатки:} Возможна неверная обработка сложносоставных слов или аббревиатур (например, <<С.-Петербург>>).

\subsection*{Результаты и анализ производительности}
\begin{itemize}
    \item Количество токенов: \textbf{81 326 141}
    \item Средняя длина токена: \textbf{11.90}
    \item Скорость токенизации: \textbf{19.45 мБ/с}
\end{itemize}

\pagebreak

\subsection*{Анализ распределения: Закон Ципфа}
Для анализа частотного распределения терминов в корпусе был построен график в логарифмических координатах (Рис. 3). На график наложено эмпирическое распределение и теоретическая кривая Закона Ципфа.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{img/3.png}
    \caption{Распределение частот токенов в логарифмических координатах}
\end{figure}

График демонстрирует, что распределение в целом следует закону Ципфа, однако наблюдаются характерные отклонения от идеальной модели:
\begin{itemize}
    \item \textbf{<<Голова>> (начало графика):} Небольшое количество самых частотных терминов встречаются чаще, чем предсказывает теория. Это объясняется тематической однородностью корпуса (медицина), где ключевые термины (<<пациент>>, <<лечение>>) доминируют.
    \item \textbf{<<Тело>> (середина графика):} Эмпирическая кривая идет почти параллельно теоретической, что подтверждает общую закономерность.
    \item \textbf{<<Хвост>> (конец графика):} Наблюдается большое количество слов с очень низкой частотой, включая множество слов, встретившихся всего один раз (гапаксы). Это приводит к появлению характерных <<ступеней>> на графике и является фундаментальным свойством естественного языка.
\end{itemize}

\subsection*{Нормализация: Стемминг}

Для нормализации токенов и сокращения словаря был реализован и интегрирован в токенизатор стеммер Портера для русского языка. Стемминг применяется к каждому токену после приведения к нижнему регистру и до проверки на валидность.

Поскольку полноценная поисковая система еще не реализована, оценка качества проводилась качественно, на основе анализа гипотетических запросов.

\textbf{Улучшение качества:} Стемминг значительно повышает полноту поиска. Например, запрос \texttt{[современная терапия]} после обработки превратится в \texttt{[современ терап]}. Такой запрос найдет документы, содержащие словосочетания <<современной терапии>>, <<современную терапию>> и т.д., которые не были бы найдены без нормализации.

\textbf{Ухудшение качества (потеря точности):} Агрессивный характер стемминга может приводить к ложным срабатываниям. Например, стеммер может ошибочно свести разные по смыслу слова к одной основе. Гипотетический пример: слова <<универсальный>> и <<университет>> могут быть сведены к общей основе <<универс>>, что приведет к появлению нерелевантных документов в выдаче.

\textbf{Способы улучшения:} Для решения проблемы потери точности можно использовать более сложный метод нормализации - лемматизацию, которая приводит слово к его нормальной словарной форме (лемме) с использованием словарей. Однако этот метод значительно медленнее стемминга.


\section*{Исходный код}
\href{https://github.com/tng00/information-retrieval/tree/main/lab3}{https://github.com/tng00/information-retrieval/tree/main/lab3}

\section*{Выводы}
В этой работе я создал ключевые компоненты для лингвистической обработки текста. Реализация токенизатора и последующий частотный анализ по закону Ципфа позволили на практике увидеть статистические закономерности реального текстового корпуса. Интеграция стеммера Портера продемонстрировала компромисс между повышением полноты поиска и риском снижения его точности. 
\newpage
